pub fn conversion(a: u32, b: u32) -> u32 {
    // XOR をとると異なるビットのみが 1 になる
    let mut diff = a ^ b;
    let mut count = 0;
    // 各ビットをチェックしてカウント
    while diff != 0 {
        count += diff & 1;
        diff >>= 1;
    }
    count
}
/*
時間計算量: O(b)

ここで b はビット数（たとえば 32 ビット固定なら 32）です。XORの後、ビット単位で1ずつシフト＆チェックします。

空間計算量: O(1)

補助変数は定数個のみ使用します。
*/

/*
diff & 1 がなぜ最下位ビットを検出できるのか、以下のように理解するとわかりやすいです。

ビットごとの AND 演算 (&)

AND 演算は、対応するビット同士が両方とも 1 の場合にだけ結果のビットが 1 になります。それ以外は 0 です。

真理値表（1ビット分だけ示すと）：

diff のビット	マスク(1)のビット	diff & 1 の結果
0	1	0
1	1	1
マスクとしての 1 (0b...0001)

整数 1 を 2進数で書くと ...0001 です（最下位ビットだけが 1）。

diff & 1 は「diff の全ビットと 0b...0001 を AND する」という意味なので、

最下位ビット同士の AND → diff の LSB がそのまま残る（1なら1、0なら0）。

他のビットは diff 側が何であってもマスク側が 0 なので、すべて 0 になる。

結果として得られる値

diff の LSB が 1 なら diff & 1 の結果も整数 1。

LSB が 0 なら結果は 0。

よって count += diff & 1; は「LSB が 1 のときにだけ count を 1 増やす」ことと同じ意味になります。
*/